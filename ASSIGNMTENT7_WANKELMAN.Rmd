---
title: "ASSIGNMENT7_WANKELMAN"
output: pdf_document
date: "2024-03-11"
---
Topic models, specifically Latent Dirichlet Allocation (LDA), unveil a nuanced approach to understanding document structure within a probabilistic framework. LDA conceptualizes a document as a mixture of various topics, where a word distribution defines each topic. This model is predicated on a generative process for creating documents, involving the selection of a mixture of issues and, subsequently, for each word in the document, choosing a topic from this mixture followed by the choice of a word from the selected topic's distribution. This abstraction allows documents to be analyzed based on thematic content, facilitating a more structured approach to text data analysis.

The utility of LDA lies in its ability to reduce the dimensionality of text data, revealing hidden thematic structures, aiding in document comparison and clustering, and enhancing information retrieval and filtering processes. By abstracting documents to thematic representations, LDA simplifies complex text data, making it manageable and interpretable. This model can significantly impact various analytical tasks, such as identifying prevalent themes across a corpus, tracking thematic changes over time, discerning authors' interests, and organizing documents into thematic clusters. These capabilities position LDA as a potent tool for comprehensive content analysis, trend analysis, authorship analysis, and document classification, offering profound insights across diverse fields.

However, LDA's methodology has challenges. Its bag-of-words approach ignores word order, which can be critical for comprehending the whole meaning of texts. The coherence and interpretability of the topics generated by LDA can vary, particularly with suboptimal parameter selection or in datasets with high diversity. The model's reliance on large datasets for meaningful topic extraction and the inherent difficulties in determining the optimal number of topics present additional hurdles. 
Consequently, LDA may need to be better suited for tasks that demand an understanding of the nuances of language that depend on word order or context. Specifically, LDA may need more syntax-sensitive linguistic analyses or the examination of concise texts with less pronounced thematic structures.

While LDA offers a robust framework for thematic analysis of text corpora, its application is most effective where the thematic structure is the primary focus, and the complexities of language use and syntax are secondary. Its strengths in uncovering latent thematic patterns and simplifying text data analysis are counterbalanced by limitations in handling language's syntactical and contextual intricacies. Future research and model refinement may address these challenges, broadening LDA's applicability and enhancing its analytical precision.

Grimmer, Justin, and Brandon M. Stewart. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.” Political analysis 21.3 (2013): 267–297. https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf.

Blei, D. M. (2012). Probabilistic topic models: Surveying a suite of algorithms that offer a solution to managing large document archives. ACM Computing Surveys, 45(3), Article 22. https://doi.org/10.1145/2133806.2133826


Grün, B., & Hornik, K. (2011). Topicmodels: An R package for fitting topic models. Faculty of Commerce - Papers (Archive), Faculty of Business and Law, University of Wollongong. Retrieved from https://ro.uow.edu.au/cgi/viewcontent.cgi?article=2408&context=commpapers

```{r}
install.packages("Rtsne")
```

```{r}
# Load Libraries
library(quanteda)
library(httr2)
library (qdapDictionaries)
library(jsonlite)
library(ndjson)
library(rjson)
library(dplyr)
library(tidytext)
library(quanteda.textstats)
library(readtext)
library(tm)
library(jsonview)
library(seededlda)
library(quanteda)
library(stopwords)
library(stringr)
library(topicmodels)
library(ldatuning)
library(plotly)
library(tidyr)
library(LDAvis)
library(tsne)
library(Rtsne)
```


#### Identified Subreddits:
r/environment: A broad subreddit that covers environmental news, discussions, and initiatives, where carbon pricing could be a frequent topic of discussion in the context of climate change mitigation strategies.

r/climate: Focused on climate-related news, research, and discussions, this subreddit might feature discussions on carbon pricing as a tool for reducing greenhouse gas emissions.

r/ClimateChange: Similar to r/climate, this subreddit is dedicated to discussing the science of climate change, impacts, and solutions, which likely includes carbon pricing mechanisms.

r/sustainability: This subreddit focuses on sustainability practices, technologies, and policies, where carbon pricing might be discussed as a means to promote sustainable economic practices.

r/RenewableEnergy: A place for news, discussions, and advancements in renewable energy, where the financial mechanisms such as carbon pricing that support renewable energy development might be topics of interest.

r/Economics: Given that carbon pricing involves significant economic policy discussions, this subreddit may occasionally delve into debates and discussions on the economic implications of carbon pricing strategies.

r/energy: This subreddit covers broader energy topics, including discussions on how carbon pricing affects energy markets, production, and consumption patterns.

r/Environmental_Policy: Specifically focused on environmental policies, this subreddit is an excellent place to find detailed discussions on the implementation, effectiveness, and challenges of carbon pricing as a policy tool.

```{r}
reddit_url = "https://www.reddit.com/api/v1/access_token"
key = "gphLeH6_N0YRj4eWvSzfag" # Your Client Key
secret = "5Eb94hhvgHgKEfzMxx4sOGObLYpn3w" # Your Client Secret
username = "PsychologyObvious81" # Your Reddit Username
password = "Dominate2024{}!*"

# Make the POST request with `httr2`
response = request("https://www.reddit.com/api/v1/access_token") |>
  req_auth_basic(key, secret) |>
  req_body_form(grant_type = "password", username = username, password = password) |>
  req_perform()

# Check the status code with `httr2` method
if (response$status_code == 200) {
  content_list = resp_body_json(response)
  token = content_list$access_token
} else {
  print(paste("Error with request, status code:", response$status_code))
}

#Reddit. Reddit API documentation. Retrieved from https://www.reddit.com/dev/api
```

```{r}
# Initialize a data frame 
results_df = data.frame(
  subreddit = character(),
  post_id = character(),
  post_title = character(),
  post_body = character(),
  comment_id = character(),
  comment_body = character(),
  stringsAsFactors = FALSE
)



subreddits = c("environment", "climate", "ClimateChange", "sustainability", 
                "Environmental_Policy", "Carbon", "CarbonCredits", "CarbonFootprint", "politics")
search_query = "carbon AND credits AND policy AND United States"



#To address the rate limiting issue (HTTP 429 errors) Loop through each subreddit process
for(subreddit in subreddits) {
  Sys.sleep(1)  # This will delay the pull to avoid hitting Reddit's rate limit
  url = paste0("https://www.reddit.com/r/", subreddit, "/search.json?q=", 
                URLencode(search_query), "&sort=new&limit=10")
  response = GET(url, add_headers(Authorization = paste("Bearer", token), 
                                   `User-Agent` = paste("R:reddit.api.pull:v1.0 (by /u/", username, ")")))
  
  if (status_code(response) == 200) {
    posts = fromJSON(rawToChar(response$content))
    for (post in posts$data$children) {
      Sys.sleep(1)  # Delay between requests for comments
      comments_url = paste0("https://www.reddit.com", post$data$permalink, ".json?limit=10")
      comments_response <- GET(comments_url, add_headers(Authorization = paste("Bearer", token), 
                                                        `User-Agent` = paste("R:reddit.api.pull:v1.0 (by /u/", username, ")")))
      if (status_code(comments_response) == 200) {
        comments_data = fromJSON(rawToChar(comments_response$content))
        # Process comments
        if(length(comments_data) >= 2) {
          comments = sapply(comments_data[[2]]$data$children, function(x) x$data$body, simplify = FALSE, USE.NAMES = FALSE)
          comment_ids = sapply(comments_data[[2]]$data$children, function(x) x$data$id, simplify = FALSE, USE.NAMES = FALSE)
          for (i in seq_along(comments)) {
            temp_df = data.frame(
              subreddit = subreddit,
              post_id = post$data$id,
              post_title = post$data$title,
              post_body = post$data$selftext,
              comment_id = comment_ids[[i]],
              comment_body = comments[[i]],
              stringsAsFactors = FALSE
            )
            results_df = rbind(results_df, temp_df)
          }
        }
      }
    }
  } else {
    print(paste("Request failed with status code:", status_code(response), "for subreddit: /r/", subreddit))
  }
}

#  write the results to a CSV file
write.csv(results_df, "reddit_comments_for_lda.csv", row.names = FALSE)


#Citation to figure out how to successfully do a Reddit API pullrequest
#Broman, K. (n.d.). APIs for social scientists. Bookdown. Retrieved from https://bookdown.org/paul/apis_for_social_scientists/reddit-api.html#prerequisites-15

#Korkmaz, S. (2016, July 4). Accessing Web Data (JSON) in R using httr. DataScience+. Retrieved from https://datascienceplus.com/accessing-web-data-json-in-r-using-httr/

#Wickham, H. Quickstart. httr. Retrieved from https://httr.r-lib.org/articles/quickstart.html

#Reddit. OAuth2. GitHub. Retrieved from https://github.com/reddit-archive/reddit/wiki/OAuth2

#rymur. Overview. Retrieved from https://rymur.github.io/overview

#Reddit. Reddit Developer Services. Reddit Help. Retrieved from https://support.reddithelp.com/hc/en-us/articles/14945211791892-Reddit-Developer-Services

#OpenAI. OpenAI. 03/10/2024 Retrieved from https://www.openai.com

#AlpsCode. (2021, January 11). How to use Reddit API. AlpsCode. Retrieved from https://alpscode.com/blog/how-to-use-reddit-api/

#rymur. Setup. Retrieved from https://rymur.github.io/setup 

#Learn R. Reading CSV files. Retrieved from https://learn-r.org/r-tutorial/read-csv.php

#R Documentation. write.table. Retrieved from https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table

```

```{r}
#Load CSV
carbon_credit_reddit = read.csv("~/Desktop/JHU/Semester 2/Text to Data/Module 7/reddit_comments_for_lda.csv")

```

```{r}
head(carbon_credit_reddit, n =10)
```

```{r}
# Prepare the data in the reddit csv file
carbon_credit_reddit$text = paste(carbon_credit_reddit$post_body)

```

```{r}
?mutate
```


```{r}
# Clean and prepare the text
clean_body_text = carbon_credit_reddit %>%
  mutate(text = str_remove_all(text, "\\n")) %>% #remove all newline characters
  mutate(text = str_remove_all(text, "\\r")) %>% #removes all occurrences of \r from the text column
  mutate(text = str_remove_all(text, "\\t")) #remove all tab characters 

#https://dplyr.tidyverse.org/reference/mutate.html
#https://rdrr.io/cran/SparkR/man/mutate.html
#https://rdrr.io/. Great freaking reference!!!!!!!!!
```

```{r}
# Create a corpus
corpus_body = corpus(clean_body_text$text)
```

```{r}
# Tokenize the corpus and remove punctuation, URLs, symbols, and stopwords
tokens_body_trim = tokens(corpus_body, 
                 remove_punct = TRUE, 
                 remove_url = TRUE, 
                 remove_numbers = TRUE,
                 remove_symbols = TRUE) %>%
tokens_remove(pattern=stopwords('en'))
dfm_body = dfm(tokens_body_trim) %>%
dfm_trim(min_termfreq = 0.4, 
         termfreq_type = 'quantile',
         max_docfreq = 0.3,
         docfreq_type = "prop")


top_terms_body = dfm_body
top_terms_body
```
When utilizing the topicmodels package in R for Latent Dirichlet Allocation (LDA), the preprocessing steps and the choice of the number of topics (K) are critical. After removing some noise, the creation of a document-feature matrix from the corpus revealed significant thematic concentrations across 78 documents, with a notable sparsity of 82.55%. This process underlines how a higher K can detail nuanced topic distinctions but risks overfitting by misinterpreting noise as significant patterns. Conversely, a lower K merges related topics into broader themes, potentially overlooking subtle thematic differences. Balancing K's value is essential to capturing the corpus's diversity without compromising the model's interpretability or simplicity.

Choosing the best value for K involves balancing between capturing the diversity of the corpus and avoiding unnecessary complexity. Metrics such as perplexity and coherence scores, along with tools like LDAvis for visual inspection, can guide this choice. Considering a corpus from environmental subreddits discussing carbon credits and policy, a moderate K that captures the variety of discussions without fragmenting coherent topics excessively would be ideal. This approach allows for identifying distinct themes such as policy implications, economic effects, environmental impacts, and technological innovations.

```{r}
# The stopwords we have don't capture everything that we need to remove
# We can skip back to previous mods/weeks and recall that the stopwords library
# actually provides a number of stopword lists that we can slap together.

expanded_stopwords = c(
  stopwords(language = 'en', source = 'snowball'),
  stopwords(language = 'en', source = 'stopwords-iso'),
  stopwords(language = 'en', source = 'smart'),
  stopwords(language = 'en', source = 'marimo'),
  stopwords(language = 'en', source = 'nltk'),
  c('gt', 'us', 'get', 't', 'e', 'r', 'like', 'just')
  ) %>%
  unique()
```

```{r}
?dfm_trim
```

```{r}
?terms
```

```{r}
# Rerun the tokenize the corpus with extended stopwords
tokens_body = tokens(corpus_body, remove_punct = TRUE, 
          remove_url = TRUE, 
          remove_numbers = TRUE,
          remove_symbols = TRUE) %>%
tokens_remove(pattern = expanded_stopwords)
dfm_body = dfm(tokens_body) %>%
dfm_trim(min_termfreq = 0.5, 
         termfreq_type = 'quantile',
         max_docfreq = 0.4, 
         docfreq_type = "prop")

top_terms = topfeatures(dfm_body, 20)
print(top_terms)
```

```{r}
?LDA
```

```{r}
?textmodel_lda
```

```{r}
?convert
```

```{r}
?set.seed
```

```{r}
help(package = "topicmodels")
```


```{r}
# Convert to Document term matrix DTM 
dtm_body = convert(dfm_body, to = 'topicmodels')

#ensuring reproducibility in stochastic algorithms
lda_body_model = LDA(dtm_body, 
                     k = 15, control = list(seed = 1234)) #seed = 1234 is used to set the random number generator's when performing Latent Dirichlet Allocation (LDA) using the LDA() function. 

# Top terms in each topic, 
lda_body_terms = terms(lda_body_model, 10) 
print(lda_body_terms)

#https://cran.r-project.org/web/views/ReproducibleResearch.html

#https://www.rdocumentation.org/packages/simEd/versions/2.0.1/topics/set.seed

```

Choosing an optimal K value of 15 for the Latent Dirichlet Allocation (LDA) model was informed by a desire to capture the nuanced discussions within a corpus of environmental subreddits focused on carbon credits and policy. This selection balances detail and interpretability, avoiding overfitting while ensuring the model can discern the diverse thematic content inherent in the discussions. The preprocessing steps and the generation of a document-feature matrix highlighted the thematic richness of the corpus, showcasing a mix of economic, social, and policy-related discussions. The decision was further supported by metrics and visual inspection tools like LDAvis, which confirmed that K = 15 allowed for a comprehensive yet manageable representation of topics, ranging from policy implications and economic effects to environmental impacts and technological innovations.

The fitted LDA model with K = 15 unveiled distinct topics that encapsulate the broad spectrum of discourse surrounding carbon credits, from economic discussions marked by terms like "investment" and "revenue" to social considerations reflected in "public" and "socialists." This array of topics illustrates the community's multifaceted approach to discussing carbon credits, touching on technical, policy, and economic aspects. Interpreting these results offers a deep dive into the community's concerns, priorities, and perspectives on environmental policy and carbon credits, revealing a landscape where technological innovations, community actions, and corporate responsibilities are pivotal discussions. This analysis not only enriches the understanding of public discourse on environmental sustainability but also provides insights that can aid stakeholders in developing informed and effective strategies for climate change mitigation.

```{r}
?topicmodel::topics
```

```{r}

# Determine the optimal number of topics using LDATuning used what was demonstrated on example code
lda_body_tune = FindTopicsNumber(
  dtm_body,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 42),
  mc.cores = 2L, 
  verbose = TRUE
)

# Before I can plot I need to ensure the results contain finite values due to Error in plot.window(...) : need finite 'xlim' values I keep getting
if(all(sapply(lda_body_tune$results, is.finite))){
  plot(lda_body_tune, main = "LDA Tuning for Optimal Number of Topics")
} else {
  warning("The tuning results contain non-finite values and cannot be plotted.")
}

optimal_k = 15
lda_optimal_model = LDA(dtm_body, k = optimal_k, control = list(seed = 1234))

# View the top terms in each topic 
lda_optimal_terms = topicmodels::terms(lda_optimal_model, 15) 
print(lda_optimal_terms)

```
The provided graph is a visual output from the FindTopicsNumber function, which is used to determine the optimal number of topics (K) for an LDA model by evaluating several statistical metrics. This function assesses model fit using different values of K, ranging from 2 to 15 in this case. The graph presents four metrics for model evaluation: Griffiths2004, CaoJuan2009, Arun2010, and Deveaud2014. Each metric has its subplot, where the x-axis represents the number of topics and the y-axis represents the metric's score. The points on the plots indicate the score for each metric at each K value. Typically, the "best" number of topics according to each metric is where the plot reaches its lowest point (for Griffiths2004 and Arun2010) or a local minimum (for CaoJuan2009 and Deveaud2014), indicating the most stable and coherent topic representation within the data.

To read these results, one would look for the number of topics where the metric scores indicate either a minimum or an elbow point—a place where the slope of the curve changes significantly. For instance, in the Griffiths2004 metric, one might look for a pronounced dip in the curve, whereas for CaoJuan2009 and Deveaud2014, a local minimum or an inflection point where the descent in scores levels off or begins to rise might be indicative of an optimal number of topics. In this particular graph, while there is no universal "optimal" K across all metrics, we might consider a value of K where multiple metrics agree or make a judgment call based on the lowest or most stable values across the metrics.


#LDAvis
```{r}

# Extract the posterior distributions for topics and terms
lda_posterior = posterior(lda_optimal_model)

# Extract phi (term-topic distributions) and theta (document-topic distributions)
phi = as.matrix(lda_posterior$terms)
theta = as.matrix(lda_posterior$topics)

# Now phi and theta are correctly extracted
vocab = colnames(phi)
doc.length = slam::row_sums(dtm_body)
term.freq = slam::col_sums(dtm_body)[match(vocab, colnames(dtm_body))]

# Adjust your createJSON or any subsequent function accordingly
json = createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc.length,
  term.frequency = term.freq, 
)


```

```{r}
serVis(json)
```

```{r}

# If lda_body_terms is not a data frame but a matrix, you need to convert it first
terms_df = as.data.frame(lda_body_terms) # Ensure this is a data frame

# If lda_body_terms is a list, the approach needs to be adjusted

# Convert row names to a column if they represent terms
if(!"Term" %in% colnames(terms_df)) {
  terms_df$Term = rownames(terms_df)
}

# Reshape data from wide to long format
melted_df = pivot_longer(terms_df, -Term, names_to = "Topic", values_to = "Word")

# Since the direct count of terms across topics for visualization as described seems to have been misunderstood,
# Let's adjust the approach for creating a histogram-like visualization with term frequencies by topic

# Assuming we want to count the occurrence of each term across topics and visualize them
term_freq = melted_df %>%
  group_by(Word) %>%
  summarise(Frequency = n()) %>%
  ungroup() %>%
  arrange(desc(Frequency))

# Plotting
ggplot(term_freq, aes(x = reorder(Word, -Frequency), y = Frequency, fill = Word)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +
  labs(x = "Term", y = "Frequency across Topics", title = "Frequency of Top Terms Across Topics")

```


```{r}
library(ggplot2)
library(tidyr) # For pivot_longer

# Assuming melted_df is already your prepared data frame

ggplot(melted_df, aes(x = Topic, y = Count, fill = Term)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.text = element_text(size = 6), 
        legend.key.size = unit(0.9, "lines")) + 
  labs(x = "Topic", y = "Count", title = "Distribution of Top Terms per Topic", fill = "Term") +
  coord_flip() 
```


#Topic 1
```{r}
# Beta matrix is representing the probabilities of terms in each topic
beta_body_matrix = lda_body_model@beta

#https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf

#https://www.rdocumentation.org/packages/SpiecEasi/versions/1.0.7/topics/symBeta

#https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/matrix

# Ensure the matrix is in the correct orientation
if (nrow(beta_body_matrix) != length(lda_body_model@terms)) {
  beta_body_matrix = t(beta_body_matrix)
}

# Assign row names to the beta matrix using the terms from the LDA model
rownames(beta_body_matrix) = lda_body_model@terms

# Plotting the top 10 terms for Topic 1
topic_number = 1
top_n = 10
topic_term_probabilities = beta_body_matrix[, topic_number, drop = FALSE]
top_terms_indices = order(topic_term_probabilities, decreasing = TRUE)[1:top_n]
top_terms = rownames(beta_body_matrix)[top_terms_indices]
top_probabilities = topic_term_probabilities[top_terms_indices]

# Create a data frame for plotting
terms_df = data.frame(
  Term = top_terms,
  Frequency = top_probabilities
)

# Plot the frequencies
ggplot(terms_df, aes(x = Term, y = Frequency, fill = Term)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Term", y = "Probability", title = paste("Top", top_n, "Term Probabilities in Topic", topic_number)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()  # Flip coordinates to make it easier to read terms

```


```{r}


# Extract the document-topic matrix from the LDA model
theta_body_matrix = topics(lda_body_model, 1)

# Get the document indices
topic_1_documents = which(theta_body_matrix == 1)

# Filter the dfm for documents associated with Topic
dfm_topic_1 = dfm_body[topic_1_documents, ]

# Calculate the frequency of each term in Topic 2 documents
term_freq_topic_1 = textstat_frequency(dfm_body)

# Get the top 10 terms based on frequency
top_terms_topic_1 = head(term_freq_topic_1, 10)

# Plot the frequencies using ggplot2
ggplot(top_terms_topic_1, aes(x = feature, y = frequency, fill = feature)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Term", y = "Frequency", title = "Top 10 Term Frequencies in Topic 1") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()  

#I deleting the other attempts at this code because I keep running into errors
```

```{r}
# beta matrix is to representing the probabilities of terms in each topic
beta_body_matrix = lda_body_model@beta

#https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf

# Ensure the matrix is in the correct orientation: terms as rows and topics as columns
if (nrow(beta_body_matrix) != length(lda_body_model@terms)) {
  beta_body_matrix = t(beta_body_matrix)
}

# Assign row names to the beta matrix using the terms from the LDA model
rownames(beta_body_matrix) = lda_body_model@terms


# Plotting the top 10 terms for Topic 2
topic_number = 2
top_n = 10
topic_term_probabilities = beta_body_matrix[, topic_number, drop = FALSE]
top_terms_indices = order(topic_term_probabilities, decreasing = TRUE)[1:top_n]
top_terms = rownames(beta_body_matrix)[top_terms_indices]
top_probabilities = topic_term_probabilities[top_terms_indices]

# Create a data frame for plotting
terms_df = data.frame(
  Term = top_terms,
  Frequency = top_probabilities
)

# Plot the frequencies using ggplot2
ggplot(terms_df, aes(x = Term, y = Frequency, fill = Term)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Term", y = "Probability", title = paste("Top", top_n, "Term Probabilities in Topic", topic_number)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()  # Flip coordinates to make it easier to read terms

```

```{r}
# beta matrix is to representing the probabilities of terms in each topic
beta_body_matrix = lda_body_model@beta

#https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf

# Ensure the matrix is in the correct orientation: terms as rows and topics as columns
if (nrow(beta_body_matrix) != length(lda_body_model@terms)) {
  beta_body_matrix = t(beta_body_matrix)
}

# Assign row names to the beta matrix using the terms from the LDA model
rownames(beta_body_matrix) = lda_body_model@terms

# Plotting the top 10 terms for Topic 3
topic_number = 3
top_n = 10
topic_term_probabilities = beta_body_matrix[, topic_number, drop = FALSE]
top_terms_indices = order(topic_term_probabilities, decreasing = TRUE)[1:top_n]
top_terms = rownames(beta_body_matrix)[top_terms_indices]
top_probabilities = topic_term_probabilities[top_terms_indices]

# Create a data frame for plotting
terms_df = data.frame(
  Term = top_terms,
  Frequency = top_probabilities
)

# Plot the frequencies using ggplot2
ggplot(terms_df, aes(x = Term, y = Frequency, fill = Term)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Term", y = "Probability", title = paste("Top", top_n, "Term Probabilities in Topic", topic_number)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()  # Flip coordinates to make it easier to read terms

```
```{r}
# beta matrix is to representing the probabilities of terms in each topic
beta_body_matrix = lda_body_model@beta

#https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf

# Ensure the matrix is in the correct orientation: terms as rows and topics as columns
if (nrow(beta_body_matrix) != length(lda_body_model@terms)) {
  beta_body_matrix = t(beta_body_matrix)
}

# Assign row names to the beta matrix using the terms from the LDA model
rownames(beta_body_matrix) = lda_body_model@terms


# Plotting the top 10 terms for Topic 4
topic_number = 4
top_n = 10
topic_term_probabilities = beta_body_matrix[, topic_number, drop = FALSE]
top_terms_indices = order(topic_term_probabilities, decreasing = TRUE)[1:top_n]
top_terms = rownames(beta_body_matrix)[top_terms_indices]
top_probabilities = topic_term_probabilities[top_terms_indices]

# Create a data frame for plotting
terms_df = data.frame(
  Term = top_terms,
  Frequency = top_probabilities
)

# Plot the frequencies using ggplot2
ggplot(terms_df, aes(x = Term, y = Frequency, fill = Term)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Term", y = "Probability", title = paste("Top", top_n, "Term Probabilities in Topic", topic_number)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()  # Flip coordinates to make it easier to read terms

```


```{r}

```

```{r}

```
